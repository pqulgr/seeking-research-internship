import streamlit as st
import cv2
import numpy as np
from mediapipe import solutions
from tensorflow import keras
import matplotlib.pyplot as plt
from io import BytesIO
import base64

# CSS pour redimensionner les √©l√©ments de la cam√©ra et mettre en valeur les r√©sultats
st.markdown("""
    <style>
        .stCamera > div > div > video {
            max-height: 200px !important;
        }
        .stCamera > div > div > div {
            max-height: 200px !important;
        }
        .result-box {
            background-color: #f0f2f6;
            border-radius: 10px;
            padding: 20px;
            margin-top: 20px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .emotion-result {
            font-size: 24px;
            font-weight: bold;
            color: #1f77b4;
            margin-bottom: 10px;
        }
        .confidence-result {
            font-size: 18px;
            color: #2ca02c;
        }
    </style>
    """, unsafe_allow_html=True)

st.title("D√©tection d'expression faciale √† la demande")

st.write("""
    Comment √ßa marche ?
    Le mod√®le compare une photo avec une expression neutre √† une photo avec une expression √©motionnelle 
    pour pr√©dire l'√©motion sur votre visage.

    Le mod√®le a √©t√© construit pendant la semaine Challenge √† l'IMT-Nord-Europe, avec 
    Louison Ribouchon et Bastien Dejardin.
""")

VECTOR_SAMPLE_RATIO = 0.7

EXPRESSION_LABELS = {
    0: ("Happy", "üôÇ"),
    1: ("Fear", "üò®"),
    2: ("Surprise", "üòÆ"),
    3: ("Anger", "üò†"),
    4: ("Disgust", "ü§¢"),
    5: ("Sad", "üò¢")
}

@st.cache_resource
def load_keras_model():
    try:
        model = keras.models.load_model('./models/face_expression_detection.h5', compile=False)
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        return model
    except Exception as e:
        st.error(f"Erreur lors du chargement du mod√®le : {str(e)}")
        return None

model = load_keras_model()

mp_face_mesh = solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)

@st.cache_data
def extract_contours_landmarks(landmarks):
    CONTOURS_INDICES = list(solutions.face_mesh.FACEMESH_CONTOURS)
    CONTOURS_INDICES = np.unique(CONTOURS_INDICES)
    CONTOURS_INDICES = CONTOURS_INDICES[CONTOURS_INDICES < len(landmarks)]
    contours_landmarks = landmarks[CONTOURS_INDICES]
    return contours_landmarks

def process_image(image):
    # Minimise le pr√©traitement, utilise l'image directement
    results = face_mesh.process(image)
    if not results.multi_face_landmarks:
        return None
    landmark_list = [(landmark.x, landmark.y, landmark.z) for landmark in results.multi_face_landmarks[0].landmark]
    landmark_list_changed = np.array(landmark_list).reshape(len(landmark_list), -1)
    return landmark_list_changed

@st.cache_data
def landmarks_contours_to_vec(neutral, expression):
    X_ = np.array(extract_contours_landmarks(expression - neutral))
    X_ = 1000*X_
    return X_.reshape(1, -1)

def predict_expression(neutral_landmarks, expression_landmarks):
    X = landmarks_contours_to_vec(neutral_landmarks, expression_landmarks)
    if X is not None and model is not None:
        result = model.predict(X, verbose=0)
        predicted_class = np.argmax(result[0])
        confidence = result[0][predicted_class]
        return predicted_class, confidence
    return None, None

def plot_vector_visualization(matrice):
    fig, ax = plt.subplots(figsize=(8, 8), dpi=100)
    
    num_vectors = len(matrice)
    sample_size = int(num_vectors * VECTOR_SAMPLE_RATIO)
    sampled_indices = np.random.choice(num_vectors, sample_size, replace=False)
    
    for i in sampled_indices:
        ax.quiver(0, 0, matrice[i][0], matrice[i][1], scale=1, scale_units='xy', angles='xy', color='b', width=0.002)
    
    ax.set_xlim(np.min(matrice), np.max(matrice))
    ax.set_ylim(np.min(matrice), np.max(matrice))
    ax.grid(True)
    ax.set_title("Vecteurs d'entr√©e (√©chantillon)", fontsize=14)
    plt.tight_layout()
    
    buf = BytesIO()
    fig.savefig(buf, format="png")
    plt.close(fig)
    return buf


def display_prediction(prediction, confidence):
    if prediction is not None and confidence is not None:
        label, emoji = EXPRESSION_LABELS[prediction]
        st.markdown(f"""
        <div class="result-box">
            <div class="emotion-result">Expression d√©tect√©e: {label} {emoji}</div>
            <div class="confidence-result">Confiance: {confidence:.2f}</div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.info("En attente de d√©tection...")


def visualize_landmarks(image, landmarks):
    h, w = image.shape[:2]
    landmark_image = image.copy()
    for landmark in landmarks:
        x, y = int(landmark[0] * w), int(landmark[1] * h)
        cv2.circle(landmark_image, (x, y), 2, (0, 255, 0), -1)
    return landmark_image

# Modify the process_image function to return both landmarks and the visualization
def process_image(image):
    results = face_mesh.process(image)
    if not results.multi_face_landmarks:
        return None, None
    landmark_list = [(landmark.x, landmark.y, landmark.z) for landmark in results.multi_face_landmarks[0].landmark]
    landmark_list_changed = np.array(landmark_list).reshape(len(landmark_list), -1)
    landmark_visualization = visualize_landmarks(image, landmark_list_changed)
    return landmark_list_changed, landmark_visualization

# Add explanation about vectors and landmarks
st.markdown("""
## Comprendre les vecteurs et les points de rep√®re (landmarks)

Les vecteurs utilis√©s dans ce mod√®le sont des repr√©sentations math√©matiques des mouvements du visage. Chaque vecteur d√©crit le d√©placement d'un point de rep√®re sp√©cifique entre l'expression neutre et l'expression √©motionnelle.

Les points de rep√®re (landmarks) sont des points cl√©s sur le visage identifi√©s par MediaPipe. Ils incluent des points autour des yeux, du nez, de la bouche et du contour du visage. En comparant la position de ces points entre deux expressions, nous pouvons capturer les subtils changements qui caract√©risent diff√©rentes √©motions.
""")


def main():
    col1, col2 = st.columns(2)

    with col1:
        st.write("Prenez une photo avec une expression neutre")
        neutral_photo = st.camera_input("Expression neutre", key="neutral")
    
    with col2:
        st.write("Prenez une photo en exprimant une √©motion")
        expression_photo = st.camera_input("Expression √©motionnelle", key="expression")

    if neutral_photo is not None and expression_photo is not None:
        col3, col4 = st.columns(2)
        
        with col3:
            st.image(neutral_photo, width=150, caption="Expression neutre")
        with col4:
            st.image(expression_photo, width=150, caption="Expression √©motionnelle")

        neutral_image = cv2.imdecode(np.frombuffer(neutral_photo.getvalue(), np.uint8), cv2.IMREAD_COLOR)
        expression_image = cv2.imdecode(np.frombuffer(expression_photo.getvalue(), np.uint8), cv2.IMREAD_COLOR)

        neutral_landmarks, neutral_visualization = process_image(neutral_image)
        expression_landmarks, expression_visualization = process_image(expression_image)

        if neutral_landmarks is not None and expression_landmarks is not None:
            st.subheader("Visualisation des points de rep√®re (landmarks)")
            col5, col6 = st.columns(2)
            with col5:
                st.image(neutral_visualization, caption="Points de rep√®re - Expression neutre", use_column_width=True)
            with col6:
                st.image(expression_visualization, caption="Points de rep√®re - Expression √©motionnelle", use_column_width=True)

            st.markdown("""
            ### Explication des points de rep√®re extraits
            
            Les points verts sur les images ci-dessus repr√©sentent les points de rep√®re d√©tect√©s par MediaPipe. 
            Nous utilisons sp√©cifiquement les points de contour du visage pour notre analyse. Ces points capturent 
            les changements subtils dans la forme du visage entre l'expression neutre et l'expression √©motionnelle.
            
            Le mod√®le utilise la diff√©rence de position de ces points entre les deux expressions pour d√©tecter l'√©motion.
            """)

            prediction, confidence = predict_expression(neutral_landmarks, expression_landmarks)
            display_prediction(prediction, confidence)
            
            vector_visualization = plot_vector_visualization(extract_contours_landmarks(expression_landmarks - neutral_landmarks))
            st.markdown(
                f"""
                <div style="display: flex; justify-content: center;">
                    <img src="data:image/png;base64,{base64.b64encode(vector_visualization.getvalue()).decode()}" alt="Visualisation des vecteurs" width="600"/>
                </div>
                """, 
                unsafe_allow_html=True
            )


            st.markdown("""
            ### Interpr√©tation des vecteurs
            
            Les fl√®ches bleues dans le graphique ci-dessus repr√©sentent les vecteurs de mouvement des points de rep√®re.
            Chaque fl√®che montre comment un point sp√©cifique s'est d√©plac√© entre l'expression neutre et l'expression √©motionnelle.
            La longueur et la direction de ces fl√®ches aident le mod√®le √† identifier l'√©motion exprim√©e.
            """)

            st.markdown("""
            ### Ca ne fonctionne pas bien ? Rien de tr√®s √©tonnant.
            Ce mod√®le √† √©t√© d√©volpp√© pour gagner une comp√©tition kaggle, et non pour √™tre d√©ploy√© en production. Notre avantage sur les autres √©quipes √† √©t√© d'utiliser la repr√©sentation en vecteur des entr√©es, ce qui nous a permis d'am√©liorer :
            - L'information contenue dans les entr√©es
            - L'explicabilit√© des r√©sultats (visualisation des vecteurs)
            - r√©duire la taille des entr√©es

            ### Pour am√©liorer, on fait comment ? 
            - une meilleure gestion des donn√©es d'entrainement, ici on a rien choisit / control√©. On peut utiliser par exemple des techniques d'augmentation de donn√©e.
            - le model est d√©pendant de la position de la t√™te dans l'image (ex. droite gauche) ce qui ne posait pas de probl√®me pour kaggle ! Mais en production, c'est un probl√®me.
            """)
        else:
            st.error("Impossible de d√©tecter les points de rep√®re sur une ou les deux photos.")

if __name__ == "__main__":
    main()